% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Introduction2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
In the field of educational measurement, a test is a collection of items developed to measure students' abilities. In order to make different measurements comparable, tests should be designed and developed providing evidence of fairness, reliability and validity \parencite{AERA2014}. These concepts have evolved together with the development of new test theories but they all agree that testing environment and procedures must be controlled in such a way differences among testing conditions do not influence the scores. In this framework, the test assembly process plays a fondamental role because, by selecting items from an item pool to build test forms conform to content and psychometric specifications, it allows to control the entire protocol of the test production, from the item construction, because the features of the pool depend on the requirements on the final tests, to the final test building: the core of test assembly. 

Large testing programs, have better access to resources like sophisticated item banking systems, opening the possibility to improve their test assembly process by means of automated test assembly (ATA). ATA has several advantages over manual test assembly. First of all, the test specifications should be defined rigorously, early on reducing the need to repeat some phases of the test development. More importantly, ATA is the only way to find optimal or near-optimal combinations of items starting from large item banks, for which manual assembly is not feasible due to the large number of possible solutions. As a consequence, ATA is fundamental to make measurements comparable while reducing operational costs. 

In practice the ATA models are not always easy to solve because they involve a very large number of decision variables and constraints. Moreover, with the standard form suggested in \textcite{VanDerLinden2005} it's not possible to define any part of the model in a non-linear manner unless one resorts to approximations. Another limit of classic optimization models, until now applied for ATA, is that they consider each variable as fixed or known. This may be valid for prices, quantities but not for estimates, key elements in ATA models. Item response theory (IRT) item parameters and subsequently the item information function (IIF) are an example.

\section{Main contributions and outline of the dissertation}

The restrictions mentioned in the last paragraph motivated us to find an alternative way to specify and solve ATA models.
For the first point we suggested a test assembly model based on the \emph{chance-constrained} (CC) approach \parencite[see][]{charnes1958cost} which allows to maximize the $\alpha$-quantile of the sampling distribution of the test information function (TIF).
The proposed model is an extension of the classical MAXIMIN ATA model (\textcite{VanDerLinden2005}, p.69-70) in which the minimum TIF among all the tests is maximized.
The sampling distribution of the TIF is obtained by boostrapping the calibration process \parencite{efron1993, shao2012}. In this way we ensure that, independently on the situation in which the calibration has been made, we have an high probability to have a certain error (possibly low) in the ability estimation.
For solving the ATA models, chance-constrained or not, we developed an algorithm based on a stochastic heuristic called \emph{simulated annealing} (SA) proposed by \textcite{goffe1996simann}.
This technique can handle large-scale models and non-linear functions.
Thanks to the Lagrange relaxation and to a random variable that accepts or rejects an inferior solution, the interesting properties of our algorithm are essentially two: it can always find the most feasible set of solutions in a limited time interval and it tries to avoid local optima seeking for the globally optimal set of test forms.

The proposed chance-constrained model, as already mentioned, is based on the empirical distribution of the TIFs of the assembled tests. This element is constructed starting from the calibration process, that is the procedure in which the IRT item parameters, such as discrimination and easiness, are estimated.
More specifically, a bootstrap is performed resampling the response data and generating a sample of estimates for each item parameter.
At the end of this phase, the item information function (IIF) for all the items in the pool, at a predefined ability point, is computed using the bootstrap samples.
These quantities are then used in the chance-constrained model to compute the $\alpha$-quantiles of the TIFs and the model is optimized by looking for the best combination of items which produces the tests with the highest quantiles. 
The use of the bootstrap in the calibration is not only propaedeutic for our test assembly model but it is also a method to retrieve additional information on the variability of the item parameters without affecting their native underlying interdependence\footnote{We do not sample independently from empirical distribution of each item parameter, instead, we resample the data following the bootstrap principle and we let the calibration algorithm produce directly the TIFs samples.This procedure should preserve the natural relationships between the item parameters.}. 

The algorithms described in this work are coded in Julia \parencite{bezanson2017julia}, this choice has been made to achieve the best performance in numerical analysis and for the availability of a lot of valuable and customizable packages for optimization.
Two packages have been developed and are available upon request\footnote{\url{https://github.com/giadasp/IRTCalibration.jl}, \url{https://github.com/giadasp/ATA.jl}}.
The first \texttt{IRTCalibration} allows to calibrate an item pool following a unidimensional latent regression model with the one-parameter logistic or two-parameter logistic models, respectively 1PL or 2PL estimated on dichotomous response data.
\texttt{IRTCalibration} uses the techniques described in Chapter \ref{sec:Julia}.
Futhermore, it provides the bootstrap tool to derive the sampling distribution functions of the item parameters.
The second package, \texttt{ATA.jl}, implements the algorithms explained in Chapter \ref{sec:CC} to assemble tests optimizing the classical MAXIMIN model \eqref{eq:MMAXIMIN} and the proposed MAXIMIN chance-constrained model \eqref{eq:CCMAXIMINapprox}.

Since, nowadays, several test assembly models are either based on classical test theory (CTT) or IRT, the dissertation starts, in Chapter \ref{sec:TestTheories}, with a brief excursus on the available test theories and continues with a introduction to the automated test assembly models.
The latter topic is enriched describing the model relaxation using the Lagrange multipliers.
It follows, in Chapter \ref{sec:Julia}, a software benchmarking, in which \texttt{Julia} is proposed as a programming language for calibrating of IRT item parameters following the 1PL and 2PL unidimensional latent variable models.
In this setting the empirical histogram method by \textcite{woods2007} is revised and a cubic-spline method is proposed to interpolate and extrapolate the observed distribution of the ability at predefined knots.
A simulation study to compare estimation accuracy and computational performance of our algorithm with respect to the \texttt{R} package \texttt{mirt} has been conducted.
Moreover, both a parametric and a non-parametric bootstrap on the calibration is provided within the software and taking the \emph{standard setup} defined in \ref{sec:Julia:simSet} we illustrate the results of applying this algorithms on the item parameter calibration by means of a simulation. 
In Chapter \ref{sec:CC}, the CC test assembly model is defined and a specific empirical version is proposed which allows to assemble parallel test forms in terms of percentiles of the TIFs.
To solve the model the SA meta-heuristic is used.
At last, the performance of the heuristic is tested in optimizing both the CC and MAXIMIN models on a simulated item pool always following the already mentioned \emph{standard setup} \ref{sec:Julia:simSet}.
Conclusions and further research ideas are provided in Chapter \ref{sec:conclusions}.
Supplementary tables and figure, which cannot be displayed floating in the body of text are reported at the end of the essay, in Appendix \ref{sec:TablesAndFigures}.

\color{black}

\tableofcontents
\pagebreak